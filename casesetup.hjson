
# there are four sizes to pick from:
#  full, mid, small, tiny
select_input_size: 'tiny'

# you can run the full 3D case or a 2D slice at mid param3
#  2D, 3D
select_dimension: '3D'

# fitting methods available:
#  'gpr.scikit': GPR by scikit-learn
#  'gpr.gpflow': GPR by GPFlow
#  'gpr.gpytorch' GPR by GPYTorch
#  'nn.dense': Neural Network by tensorflow/keras usual Dense
#  'nn.attention': Neural Network by tensorflow/keras with an Attention layer
method: 'gpr.gpflow'

# let the hyperparameters by optimised by a few methods, or just run, or just restart
#  'conventional': LML for GPR, RMSE for NN
#  'diffusionloss': my method :)
#  'nahimgood': just dry run a model, no optimisation
#  'restart': read a presaved model
if_train_optim: 'conventional'

# output figure format: eps, png
fig_format: eps

# GPR scikit options
GPR_setup:
{
    # Set to false if you want it optimised, otherwise give it a value (it will be squared)
    kernel_variance: 0.15

    # Give the kernel lengthscale a value
    #_ Used as initial for all optimisations, or fixed for the dry runs
    #_ If ARD flag is true, value will be used for all dimensions
    kernel_lengthscale: 5.0
    if_kernel_lengthscale_ARD: false

    # Scikit options
    scikit_setup:
    {
        n_restarts_optimizer: 3
    }

    # GPFlow options
    gpflow_setup:
    {
        maxiter: 1500,
        gtol: 1e-9,
        ftol: 1e-9,
    }

    # GPYTorch options
    gpytorch_setup:
    {
        maxiter: 750,
    }

    # Diffusion-loss optimizer options
    diffusionloss_minimise_setup:
    {
        maxiter: 1500,
        final_tr_radius: 1e-9,
        initial_tr_radius: 3,
    }
}

# Neural network options
#  provide the hidden layers as a vector with the number of neurons [n1, n2, ...] for as many layers
keras_setup:
{
    learning_rate: 0.002,
    epochs: 75,
    batch_size: 32,
    hidden_layers: [1024, 512],

    multiheadattention_setup:
    {
        num_heads: 2
    }
}
